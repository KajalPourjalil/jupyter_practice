{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 18500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"sample_text.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Safety has been a facade. Education for educat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>More time should be spent with education aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am a special education teacher with autistic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It has been a very challenging year and I pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I support the decision to keep students in sch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Response\n",
       "0  Safety has been a facade. Education for educat...\n",
       "1  More time should be spent with education aroun...\n",
       "2  I am a special education teacher with autistic...\n",
       "3  It has been a very challenging year and I pers...\n",
       "4  I support the decision to keep students in sch..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 1 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Response  6 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 180.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Safety has been a facade. Education for educat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Response\n",
       "count                                                   6\n",
       "unique                                                  6\n",
       "top     Safety has been a facade. Education for educat...\n",
       "freq                                                    1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to join all the responses into a single mega string, since I want to analyze the responses as a whole. For this, I use pandas’ handy cat string method to concatenate all the strings in the Reponse column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = df.Response.str.cat(sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a spaCy document with that text. I don’t need the named entity recognizer (NER) so I disable that to save on memory and computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(all_text, disable = ['ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does a few things: it splits the text into individual words and tags them with their part-of-speech, like nouns, verbs, adjectives, etc. It also recognizes common words (stop-words) like “and”, “I”, and “with” that don’t have much meaning and can be excluded from word counts.\n",
    "\n",
    "Now I can do an overall word frequency analysis to see the most common words that aren’t stop words or punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('school', 5),\n",
       " ('teacher', 5),\n",
       " ('year', 5),\n",
       " ('support', 5),\n",
       " ('education', 3),\n",
       " ('\\x85', 3),\n",
       " ('teaching', 3),\n",
       " ('health', 3),\n",
       " ('feel', 3),\n",
       " ('case', 2),\n",
       " ('student', 2),\n",
       " ('enhanced', 2),\n",
       " ('invisible', 2),\n",
       " ('aide', 2),\n",
       " ('mental', 2),\n",
       " ('way', 2),\n",
       " ('safety', 1),\n",
       " ('facade', 1),\n",
       " ('education\\x92s', 1),\n",
       " ('sake', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "word_freq = Counter(words)\n",
    "word_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I asked for the lemma_ attribute of each token, which is the lemmatized version of a word. That means that words with different variations, like “be”, “am”, “is” and “are” are all standardized to a root version like “be”. Plural words are transformed to their singular versions. Already you can get a sense of what teachers talk about the most. The most commonly-used verbs are quite telling: Feel and need. What are teachers feeling? What do they need? spaCy can help us answer this with pattern matching.\n",
    "\n",
    "Pattern matching in linguistics is a bit like regular expressions, but for language. Instead of matching a sequence of characters, you can match a sequence of word types. For example, what are the most common adjective-noun phrases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mental health', 2),\n",
       " ('more time', 1),\n",
       " ('proper mask', 1),\n",
       " ('transparent information', 1),\n",
       " ('positive cases', 1),\n",
       " ('special education', 1),\n",
       " ('autistic students', 1),\n",
       " ('enhanced cleaning', 1),\n",
       " ('medical aides', 1),\n",
       " ('quadruple duties', 1),\n",
       " ('discriminated event', 1),\n",
       " ('final year', 1),\n",
       " ('challenging year', 1),\n",
       " ('little understanding', 1),\n",
       " ('many people', 1),\n",
       " ('sure kids', 1),\n",
       " ('bigger focuses', 1),\n",
       " ('curricular outcomes', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher \n",
    "\n",
    "matcher = Matcher(nlp.vocab) \n",
    "pattern = [{'POS':'ADJ'}, {'POS':'NOUN'}] \n",
    "matcher.add('ADJ_PHRASE', [pattern]) \n",
    "\n",
    "matches = matcher(doc, as_spans=True) \n",
    "phrases = [] \n",
    "for span in matches:\n",
    "    phrases.append(span.text.lower())\n",
    "    phrase_freq = Counter(phrases)\n",
    "\n",
    "phrase_freq.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how pattern is defined: a list of dictionaries, each defining a part of speech (POS). In this case, it’s an adjective, then a noun. So spaCy will look for all instances of this pattern in the text. Incredible. “Mental health” was by far the most common phrase in this pattern written by teachers.\n",
    "\n",
    "Now let’s look for the most common adjective that follow the phrase “I am” or “I feel”. For this the pattern has to be more complex. Because these are all valid constructions that we’d like to capture:\n",
    "\n",
    "-   I feel exhausted\n",
    "-   I really feel exhausted\n",
    "-   We’re pretty exhausted\n",
    "\n",
    "for this spacy Matcher allows wildcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('invisible', 2), ('special', 1), ('alone', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feel_adj = []\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {'LOWER': {'IN': ['i', 'we']}}, {'OP': '?'},\n",
    "    {'LOWER': {'IN': ['feel', 'am', \"'m\", 'are', \"'re\"]}},\n",
    "    {'OP': '?'}, {'OP': '?'}, {'POS': 'ADJ'}\n",
    "]\n",
    "\n",
    "matcher.add('FeelAdj', [pattern])\n",
    "matches = matcher(doc, as_spans=True)\n",
    "\n",
    "for span in matches:\n",
    "    feel_adj.extend([token.lemma_ for token in span if token.pos_ == 'ADJ'])\n",
    "\n",
    "Counter(feel_adj).most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern now asks for the following: the lower-case version of “I” or “we” (so it’s case insensitive), any possible word in one or zero occurrences (operator ‘?’, like regex), the lower-case version of any of “feel”, “am”, “are” and contractions thereof, two possible filler words, and an adjective.\n",
    "\n",
    "Then it loops through the matches, looks only for the adjectives captured, and adds it to a list. This is incredibly informative. When we asked teachers to write whatever they wanted, so many expressed feeling of exhaustion, concern, and fear.\n",
    "\n",
    "Here’s a pattern that looks for phrases that start with “I/we want/need”, followed by a noun, with optional filler words in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "want_adj = []\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'LOWER' : {'IN' : ['i', 'we']}}, {'IS_ALPHA':True, 'OP': '?'},\n",
    "           {'LOWER': {'IN' : ['need', 'want']}}, {'IS_ALPHA': True, 'OP': '?'},\n",
    "           {'IS_ALPHA': True, 'OP': '?'}, {'POS': 'NOUN'}]\n",
    "\n",
    "matcher.add(\"WantPhrase\", [pattern])\n",
    "matches = matcher(doc, as_spans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another flavour of spaCy’s Matcher is the PhraseMatcher, which looks for instances of a specific phrase that you define. Let’s say I want to find the words that most frequently occur near the phrase “mental health”: Look at how I defined span : it grabs the 10 tokens before and after “mental health”. Then I strip out stop-words and count the words that remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('health', 3),\n",
       " ('aide', 2),\n",
       " ('mental', 2),\n",
       " ('work', 1),\n",
       " ('custodian', 1),\n",
       " ('medical', 1),\n",
       " ('family', 1),\n",
       " ('counsellor', 1),\n",
       " ('\\x85', 1),\n",
       " ('finally', 1),\n",
       " ('teacher', 1),\n",
       " ('absolutely', 1),\n",
       " ('vaccine', 1),\n",
       " ('effect', 1),\n",
       " ('people', 1),\n",
       " ('make', 1),\n",
       " ('sure', 1),\n",
       " ('kid', 1),\n",
       " ('happy', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "mental_health_colloc = []\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = 'LOWER') \n",
    "# The attr above ensures all instances are converted to lower-case so the search is case-insensitive\n",
    "\n",
    "pattern = [nlp.make_doc('mental health')]\n",
    "matcher.add('mentalHealth', pattern) \n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start-10 : end+10]   \n",
    "    mental_health_colloc.extend([token.lemma_.lower() for token in span if not token.is_stop and not token.is_punct]) \n",
    "\n",
    "Counter(mental_health_colloc).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
